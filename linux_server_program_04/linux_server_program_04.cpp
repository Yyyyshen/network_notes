// linux_server_program_04.cpp : 此文件包含 "main" 函数。程序执行将在此处开始并结束。
//

#include <iostream>

//
//高性能服务器程序框架
//

//
//服务器程序一般原理中，可解构三个模块
// I/O处理单元，有四种I/O模型和两种高效事件处理模式
// 逻辑单元，两种高效并发模式，以及高效逻辑处理方式——有限状态机
// 存储单元，可选模块
//

//
//服务器模型
// 
//C/S
// TCP/IP协议在设计和实现上没有客户端和服务端的概念，通信中机器是对等的
// 但资源被数据提供者拥有，所以几乎大家都自然的采用了C/S模型，所有客户端通过访问服务器获取资源
// 服务器启动后，创建监听socket（一个或多个），并调用bind函数绑定到端口上，然后调用listen开始监听
// 客户端创建连接socket后通过connect发起连接
// 服务端通过accept，加上某种I/O模型处理这些客户端连接，分配一个逻辑单元给新的连接服务（子进程、子线程或其他）
// 
//P2P
// 点对点，比C/S模型更符合网络通信实际情况，不需要服务器为中心，所有主机直接对等
// 每台机器消耗服务也提供服务，资源能充分自由地共享，但用户之间传输过多时，负载很重
// 并且存在一个问题，主机之间很难互相发现，所以通常带有一个专门地发现服务器
// 所以，也可以看作C/S模型扩展，每台主机既是客户端又是服务器
//

//
//编程框架
// 服务器基本模块功能
// 模块           单个服务器           服务器集群
// I/O处理        处理连接、读写数据   接入服务器、负载均衡
// 逻辑单元       业务进程或线程	   逻辑服务器
// 存储单元		  本地db、文件、缓存   数据库服务器
// 请求队列		  各单元通信方式	   各服务器之间的永久TCP连接
//

//
//I/O模型
// socket创建的时候默认是阻塞的
// 可以给socket函数第二个参数加上SOCK_NONBLOCK标志，或通过fcntl的F_SETFL命令设置非阻塞
// 不仅是socket，所有文件描述符都可以应用这个概念，根据这一点，分为阻塞I/O和非阻塞I/O
// 
// 阻塞I/O执行的系统调用会因为无法立即完成被系统挂起，直到等待的事件发生
// 比如connect，首先发送同步报文给服务器，等待服务器返回确认报文，如果这个过程因为网络条件差等原因较慢，则会阻塞在调用处，直到收到确认报文或超时
// 
// 非阻塞I/O总是立即返回，不管事件是否发生，如果没有发生，返回-1，此时-1与错误时返回一样，需要根据errno区分
// 对于accept、send、recv，事件未发生时errno通常设置成EAGAIN或EWOULDBLOCK（值一样，表示重试或期望阻塞）
// 对于connect，一般被设置成EINPROGRESS（在处理中）
// 
// 事件发生后才处理非阻塞I/O才能提高效率，因此，非阻塞I/O需要与I/O通知机制一起用，比如I/O复用和SIGIO信号
// I/O复用常用I/O通知机制，通过复用函数向内核注册一组事件，内核通过复用函数把就绪的事件通知给应用程序
// 常用的函数有select、poll、epoll_wait，I/O复用函数本身是阻塞的，提高效率的原因是它可以同时监听多个I/O事件
// SIGIO信号也可以报告I/O事件，可以为目标fd指定宿主进程，被指定的进程将捕获SIGIO信号，当fd有事件发生，SIGIO处理函数触发
// 
// 理论上，阻塞I/O、I/O复用和信号驱动I/O都是同步I/O模型，因为读写操作都是在I/O事件发生后由应用程序完成
// POSIX规范定义的异步I/O模型中，应该是用户可以直接对I/O操作执行读写，告诉内核自己读写缓冲区的位置，接下来就是内核接管
// 同步I/O通知给应用的是就绪事件，异步I/O向应用通知的是I/O完成事件 ***
//

//
//两种高效的事件处理模式
// 服务器通常需要处理三类事件，I/O事件、信号、定时事件
// 同步I/O模型通常用于实现Reactor模式
// 异步I/O模型则用于实现Proactor模式
// 也可以使用同步I/O方式模拟出Proactor模式
// 
//Reactor
// 要求主线程（I/O处理单元）只负责监听文件描述符上是否有事件发生
// 有就立即将该事件通知工作线程（逻辑单元），除此之外主线程不做其他实质性工作
// 读写、接收新连接、处理请求等在工作线程完成
// 
// 同步I/O模型实现Reactor模式的流程（使用epoll_wait为例）
//	主线程向epoll内核事件表中注册socket读就绪事件
//	主线程调用epoll_wait等待socket上有数据可读
//	触发事件后，epoll_wait通知主线程，主线程将可读事件放入请求队列
//	请求队列上某个工作线程被唤醒，从socket读取数据，处理客户请求，然后向epoll内核事件表注册socket写事件就绪事件
//	主线程epoll_wait等待socket可写，触发时将可写事件放入请求队列
//	请求队列某工作线程被唤醒，向socket写数据，返回请求结果
// 
//Proactor
// 与Reactor模式不同，将所有I/O操作都交给主线程和内核，工作线程仅负责业务逻辑
// 
// 异步I/O模型实现Proactor模式的流程（以aio_read、aio_write为例）
//	主线程调用aio_read函数向内核注册socket上的读完成事件，告知读缓冲区位置，以及读完成时的通知方式（信号）
//	主线程继续处理其他逻辑
//	当socket数据被读入用户缓冲区后，内核通知应用程序，表示数据可用
//	应用程序通过预先定义好的处理函数选择一个工作线程处理请求
//	工作线程完成处理后，调用aio_write函数向内核注册写完成事件，告知写缓冲区位置，以及写完成时的通知方式
//	主线程继续处理其他逻辑
//	当用户缓冲区数据写入socket后，内核通知应用程序，表示数据发送完毕
//	应用程序通过预先定义好的处理函数选择一个工作线程做善后处理（继续接收或关闭socket等）
// 
//同步I/O模拟Proactor模式
// 原理是主线程执行数据读写操作，完成后，主线程通知工作线程模拟“事件完成”
// 从工作线程的角度，就是直接获取了读写结果，只需要对读写结果进行逻辑处理
// 
// 使用同步I/O模型模拟Proactor模式的流程（以epoll_wait为例）
//	主线程向epoll内核事件表注册socket读就绪事件
//	主线程调用epoll_wait等待数据可读
//	触发事件后，epoll_wait通知主线程，主线程从socket循环读取数据，完全读完后将数据封装为请求对象插入请求队列
//	请求队列上某个工作线程被唤醒，拿到请求对象做业务处理，然后向epoll内核事件表注册socket写就绪事件
//	主线程epoll_wait等待socket可写
//	触发事件后，epoll_wait通知主线程，主线程向socket上写入请求队列处理后的请求结果
//

//
//两种高效并发模式
// 并发的目的是让程序同时执行多个任务
// 如果程序计算密集，并发并没有优势，反而会因为任务切换降低效率
// 如果是I/O密集型，经常读写文件访问数据库等，则I/O速度远没有CPU速度快，阻塞在I/O的操作浪费大量CPU时间
// 当有多个执行线程，则当前线程被I/O阻塞时可主动让出CPU（或系统调度），并转移执行权到其他线程，当I/O操作完成再继续
// 
// 并发通常有多进程和多线程（现在还有协程、纤程等）
// 并发模式指I/O处理单元和多个逻辑单元之间协调完成任务的方式
// 服务器主要有两种
//	半同步/半异步（half-sync/half-async）模式
//	领导者/追随者（Leader/Followers）模式
// 
//半同步/半异步
// 此处同步异步和I/O中的同步和异步不同
// I/O中指的是内核向应用程序通知是何种I/O事件（就绪或完成），谁来完成I/O读写（应用还是内核）
// 并发中，同步指的是程序按照代码序列顺序执行，异步指的是程序执行需要系统事件来驱动（中断、信号等）
// 
// 同步方式运行的线程是同步线程，编写简单，效率相对较低，实时性差些
// 异步方式运行的线程是异步线程，逻辑相对复杂，难调试和扩展，不适合大量并发
// 服务器则既要求较好的实时性，有要求能同时处理很多客户端请求
// 就需要结合起来，同时使用两种线程，即半同步/半异步模式
// 
// 此模式中，同步线程处理客户逻辑，相当于逻辑单元
// 异步线程处理I/O事件，相当于I/O处理单元
// 异步线程监听请求，将数据封装为请求对象，插入请求队列；请求队列通知某工作在同步模式的工作线程读取并处理请求对象
// 选择哪个工作线程为请求服务则取决与请求队列设计，最简单的就是轮询调度，也可以通过条件变量或信号量随机选取
// 
// 综合考虑处理事件模式和I/O模型，则半同步/半异步存在一些变体
// 半同步/半反应堆（half-sync/half-reactive）模式 就是一种变体
//	异步线程只有一个，主线程充当，负责监听所有socket上的事件
//	监听事件有可读事件即新连接到来，主线程接受后得到新的连接socket，之后向epoll内核事件表注册该socket的读写事件
//	如果连接socket上有读写事件发生，代表客户请求到来或需要发送数据到客户端，主线程则将该socket插入请求队列
//	睡眠在请求队列中的所有工作线程通过竞争获得任务管理权（互斥锁，只有空闲线程才会来处理新任务）
// 主线程插入队列的任务是socket，说明事件处理是reactor，工作线程自己从socket上读写（也可以使用模拟的Proactor）
// 存在缺点
//	主线程和工作线程共享请求队列，主线程添加或工作线程取出需要加锁，有一定的CPU时间浪费
//	每个工作线程只能处理一个客户请求，如果连接很多，请求队列将会堆积；单纯增加工作线程数量，也会因为线程切换消耗大量CPU时间
// 
// 一种相对高效的半同步/半异步模式，每个工作线程能同时处理多个客户连接
//	主线程只负责管理监听socket，连接socket由工作线程管理
//	新连接到来时，主线程接受并将返回的socket派发给某个工作线程
//	后续直到客户连接关闭，该socket上的任何I/O操作都由被选中的工作线程处理
//	主线程派发socket最简单的方式是管道，向管道写数据，工作线程检测到管道可读，分析是否是新客户连接到来
//	如果是，则把新socket上的读写时间注册到自己的epoll内核事件表，这样当一个连接空闲时，还可以接收下一个连接
// 这个模式下，主线程和工作线程分别维护自己的事件循环，各自独立监听不同事件
// 理论上，每个线程其实都是工作在异步模式，并非严格的半同步/半异步模式
// 
//领导者/追随者模式
// 多个工作线程轮流获得事件源集合，轮流监听、分发并处理事件的一种模式
// 任意时间点，程序仅有一个领导者线程，负责监听I/O事件，其他线程都是追随者
// 休眠在线程池中的追随者等待称为新的领导者
// 当前领导者检测到I/O事件，先从线程池选出新的领导线程
// 新的领导线程继续检测I/O事件，而原来的领导者开始处理I/O事件，让出领导权
// 这样不同的线程实现了并发
// 
// 此模式组件
//	句柄集、线程集、事件处理器、具体事件处理器
// 句柄用于表示I/O资源，Linux通常就是一个fd，句柄集管理大量句柄，使用wait_for_event方法监听句柄上的I/O事件
//	就绪后，通知给领导线程，领导者调用绑定在句柄上的事件处理器来处理事件
// 线程集时所有工作线程的管理者，负责个线程之间的同步、新领导的推选
//	线程在任一事件都处于三种状态之一
//	Leader 领导者身份，负责等待句柄集上的I/O事件
//	Processing 正在处理事件，是领导者检测到事件后的状态；也可能指定其他追随者处理事件，自己依然是领导者
//	Follower 追随者，等待称为新领导或被指定完成新任务
// 事件处理器和具体事件处理器
//	通常包括一个或多个回调函数，用于处理业务逻辑，事件处理器会事先绑定到某个句柄
//	具体事件处理器则是一个派生类，处理特定任务
// 
// 领导者线程自己监听I/O事件并处理请求，因此不需要线程之间传递额外数据
// 无须在线程之间同步对请求队列的访问
// 但缺点是仅支持一个事件源集合，无法让每个工作线程独立管理多个客户连接
//

//
//有限状态机
// 除了服务器中各模块 I/O处理单元、请求队列、逻辑单元 之间的协调
// 逻辑单元内部可以使用有限状态机，是一种高效编程方法
// 应用协议层协议头常包含数据包类型字段或命令字段，每种类型可以映射为一种执行状态，根据这些状态处理不同逻辑
// （其实就是一个分类逻辑处理，没想到还有个专有名称）
// 
// 状态独立的有限状态机
/*
state_machine(Package pack)
{
	Type type = pack.getType();
	switch(type)
	{
		case type_A:
			process_A(pack);
			break;
		case type_b:
			process_B(pack);
			break;
	}
}
 */
// 简单的状态机，每个状态相互独立，没有相互转换
// 
// 带状态转移的有限状态机
/*
state_machine()
{
	State cur = type_A;
	while(cur != type_C)
	{
		switch(cur)
		{
			case type_A:
				process_A(pack);
				cur = type_B;
				break;
			case type_B:
				process_B(pack);
				cur = type_C
				break;
		}
	}
}
*/
// 每次在当前状态处理数据之后，转化状态（supershield SDK就用了这个）
// 
// HTTP协议分析
// 判断协议头结束依据是一个空行，仅包含一对回车换行符（前一行头的回车换行和空行的回车换行）
// 若一次读操作没有读入HTTP整个头部，即没有遇到空行，必须等待，再次读入
// 每次读操作后，就要分析数据中是否符合协议
// 同时，可以在寻找空行的过程中，完成对之前头字段的分析，提高解析效率
// 
// 例：使用主从两个有限状态机实现HTTP请求的读取和分析
// linux_src/http_parse.cpp
//

//
//提高性能的其他建议
// 
// 池
// 当资源充足，可以用空间换时间
// 在服务器启动之初就创建并初始化号一组资源
// 每次需要时，就从池中分配，无需每次都调用系统内存分配函数
// 使用完毕也不需要释放
// 避免了对内核的频繁访问
// 
// 数据复制
// 避免不必要的数据复制
// 例如ftp，客户端请求文件时，服务器只检测目标文件是否存在，是否有权限，不关心内容
// 则无需把目标文件读到用户缓冲区再send出去，可以使用sendfile这样零拷贝的函数
// 
// 上下文切换和锁
// 进程或线程切换导致系统开销，即使是I/O密集型，也不该有太多的工作线程
// 为每个连接都创建一个工作线程时不可取的，可以使用半同步/半异步，一个线程同时处理多个客户连接
// 并发程序另一个问题就是共享资源的加锁保护，这也是性能下降的因素
// 如果能不用锁，尽量不用锁
// 用锁时，减小锁粒度，如读写锁、利用大括号生命周期缩小使用范围等
//



//
//I/O复用
//

//
//使用场景
// I/O复用使得程序能同时监听多个fd，提高程序性能
// 客户端需要同时处理多个connect时
// 客户端程序同时处理用户输入和网络连接（聊天室）
// 服务器需要同时处理监听socket和连接socket（最常见）
// 同时处理TCP和UDP时，例如echo服务器
// 同时监听多个端口或处理多种服务时
// 
// 系统调用
// select、poll和epoll
//

//
//select
// 在指定时间内，监听用户感兴趣的fd上的I/O事件
// 
// int select(int nfds, fd_set* readfds, fd_set* writefds, fd_set* exceptfds, struct timeval* timeout);
// 参数nfds指定被监听的fd总数，通常设置为监听的所有文件描述符值中的最大值+1
// 三个fd_set分别是可读、可写、异常事件对应的文件描述符集合，当select调用返回时，内核将修改它们并通知应用程序有文件描述符就绪
// 
// fd_set结构体仅包含一个整型数组，每一位标记一个文件描述符，数量由FD_SETSIZE指定，所以select有处理上限
// 位操作比较麻烦，系统提供一系列宏处理
// FD_ZERO(fd_set * fdset);//清除set中的所有位
// FD_SET(int fd, fd_set* fdset);//设置位fd
// FD_CLR(int fd, fd_set* fdset);//清除位fd
// int FD_ISSET(int fd, fd_set* fdset);//测试fdset的位是否被设置
// 
// timeout设置超时时间，提供了秒和微秒级的定时方式
// 
// select成功时返回就绪fd的总数；超时返回0；失败返回-1，并设置errno，如收到信号，立即返回-1设置errno为EINTR
// 
//文件描述符就绪条件
// 可读情况
//	内核接收缓冲区大于等于低水位标记SO_RCVLOWAT，此时可无阻塞读socket
//	通信对方关闭，进行读操作返回0
//	监听socket上有新的连接
//	有未处理错误，可以使用getsockopt读取和清除错误
// 可写情况
//	内核发送缓冲区可用字节大于等于低水位标记SO_SNDLOWAT，此时可无阻塞写
//	写操作被关闭，对写操作被关闭的socket执行写操作（收到RST后向对端发数据）触发SIGPIPE信号
//	非阻塞connect连接成功后
//	有未处理错误，getsockopt
// 
//处理带外数据
// socket收到普通和带外数据都会使select返回，但socket处于不同状态
// 前者可读，后者则是异常
// 例：select同时处理两种数据
// linux_src/select_test.cpp
//

//
//poll
// 与select类似，在指定时间内轮询一定数量的fd，测试其中是否有就绪者
// 
// int poll(struct pollfd* fds, nfds_t nfds, int timeout);
// fds参数是一个pollfd结构类型的数组，指定所有我们感兴趣的fd发生的I/O事件
// struct pollfd
// {
//		int fd;
//		short events;//注册的事件
//		short revents;//实际发生的事件，内核填充
// }
// nfds参数指定fds集合大小
// timeout与select含义相同
//

//
//epoll
// 
//以一个内核事件表为驱动
// epoll使用一组函数完成任务而不是单个函数
// 把用户关心的fd的事件放在内核的一个事件表中，无须每次重复设置
// 但epoll需要使用一个额外的fd来位移标识内核中这个事件表
// 
// 创建这个事件表需要如下函数
// int epoll_create(int size);
// size参数不会起作用，一般传一个大于0的数即可
// 返回的fd用作其他epoll系列函数的第一个参数，以指定要访问的内核事件表
// 
// 操作这个事件表的函数
// int epoll_ctl(int epfd, int op, int fd, struct epoll_event* event);
// op参数指定操作类型，有EPOLL_CTL_ADD/MOD/DEL，也就是添加、修改和删除
// event参数指定事件，支持的事件类型和poll支持的差不多，宏名前面加'E'
// struct epoll_event
// {
//		__uint32_t events;	//epoll事件
//		epoll_data_t data;	//绑定的数据
// }
// typedef union epoll_data
// {
//		void* ptr;
//		int fd;				//最常用的是绑定对应的文件描述符
//		uint32_t u32;
//		uint64_t u64;
// } epoll_data_t
// 
// 主要监听函数
// int epoll_wait(int epfd, struct epoll_event* events, int maxevents, int timeout);
// 在一段超时时间等待一组文件描述符上的事件，成功则返回就绪的fd个数，失败返回-1并设置errno
// 如果检测到事件，就会将所有就绪的事件从内核事件表复制到它的第二个参数events指向的数组中
// 数组只用于输出就绪事件，感兴趣的事件绑定依赖于epfd
// 不像select和poll数组参数那样既用于传入用户注册事件又用于输出就绪事件
// 提高了程序索引就绪fd的效率
// 
//模式 LT 和 ET
// epoll对fd的操作模式有两种
//	LT 默认的工作模式，相当于一个较高效率的poll；检测到有事件发生时，通知应用，应用可以不处理，下次调用还会再次通知此事件，直到处理
//	ET 是epoll的高效工作模式；此模式下，事件发生并通知后，如果应用不处理，下次调用wait则不会再通知，降低了触发次数，效率相对较高
// 例：两种模式的区别
// linux_src/epoll_lt_and_et.cpp
// 
//EPOLLONESHOT事件
// 不管ET还是LT，某事件还是可能多次触发的
// 并发环境下，如果多个线程操作一个socket上数据，同步问题就会很麻烦
// 可以使用EPOLLONESHOT事件，实现任意时刻只有一个线程能够处理这个socket
// 当这个线程处理完这次socket事件，则再次对其设置EPOLLONESHOT，以便继续触发被其他线程处理
// 例：EPOLLONESHOT使用
// linux_src/epoll_oneshot_test.cpp
//

//
//三组I/O复用函数的比较
// 
//select
// 三组事件集合，每次需要重置，索引就绪事件描述符事件复杂度O(n)，有最大支持数限制，工作模式LT，内核采用轮询方式检测就绪事件，复杂度O(n)
// 
//poll
// 一个事件集合，内核同样通过修改集合内事件状态进行反馈，索引O(n)，无最大支持数限制（fd上限65535），LT模式，内核轮询检测就绪事件，O(n)
// 
//epoll
// 内核通过一个事件表接管用户注册的事件，无须每次重复传入，输入输出的事件集合分开，索引O(1)，无支持数限制，支持ET高效模式，内核通过回调检测就绪事件，O(1)
//

int main()
{
	std::cout << "Hello World!\n";
}
