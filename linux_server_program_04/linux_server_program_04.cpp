// linux_server_program_04.cpp : 此文件包含 "main" 函数。程序执行将在此处开始并结束。
//

#include <iostream>

//
//高性能服务器程序框架
//

//
//服务器程序一般原理中，可解构三个模块
// I/O处理单元，有四种I/O模型和两种高效事件处理模式
// 逻辑单元，两种高效并发模式，以及高效逻辑处理方式——有限状态机
// 存储单元，可选模块
//

//
//服务器模型
// 
//C/S
// TCP/IP协议在设计和实现上没有客户端和服务端的概念，通信中机器是对等的
// 但资源被数据提供者拥有，所以几乎大家都自然的采用了C/S模型，所有客户端通过访问服务器获取资源
// 服务器启动后，创建监听socket（一个或多个），并调用bind函数绑定到端口上，然后调用listen开始监听
// 客户端创建连接socket后通过connect发起连接
// 服务端通过accept，加上某种I/O模型处理这些客户端连接，分配一个逻辑单元给新的连接服务（子进程、子线程或其他）
// 
//P2P
// 点对点，比C/S模型更符合网络通信实际情况，不需要服务器为中心，所有主机直接对等
// 每台机器消耗服务也提供服务，资源能充分自由地共享，但用户之间传输过多时，负载很重
// 并且存在一个问题，主机之间很难互相发现，所以通常带有一个专门地发现服务器
// 所以，也可以看作C/S模型扩展，每台主机既是客户端又是服务器
//

//
//编程框架
// 服务器基本模块功能
// 模块           单个服务器           服务器集群
// I/O处理        处理连接、读写数据   接入服务器、负载均衡
// 逻辑单元       业务进程或线程	   逻辑服务器
// 存储单元		  本地db、文件、缓存   数据库服务器
// 请求队列		  各单元通信方式	   各服务器之间的永久TCP连接
//

//
//I/O模型
// socket创建的时候默认是阻塞的
// 可以给socket函数第二个参数加上SOCK_NONBLOCK标志，或通过fcntl的F_SETFL命令设置非阻塞
// 不仅是socket，所有文件描述符都可以应用这个概念，根据这一点，分为阻塞I/O和非阻塞I/O
// 
// 阻塞I/O执行的系统调用会因为无法立即完成被系统挂起，直到等待的事件发生
// 比如connect，首先发送同步报文给服务器，等待服务器返回确认报文，如果这个过程因为网络条件差等原因较慢，则会阻塞在调用处，直到收到确认报文或超时
// 
// 非阻塞I/O总是立即返回，不管事件是否发生，如果没有发生，返回-1，此时-1与错误时返回一样，需要根据errno区分
// 对于accept、send、recv，事件未发生时errno通常设置成EAGAIN或EWOULDBLOCK（值一样，表示重试或期望阻塞）
// 对于connect，一般被设置成EINPROGRESS（在处理中）
// 
// 事件发生后才处理非阻塞I/O才能提高效率，因此，非阻塞I/O需要与I/O通知机制一起用，比如I/O复用和SIGIO信号
// I/O复用常用I/O通知机制，通过复用函数向内核注册一组事件，内核通过复用函数把就绪的事件通知给应用程序
// 常用的函数有select、poll、epoll_wait，I/O复用函数本身是阻塞的，提高效率的原因是它可以同时监听多个I/O事件
// SIGIO信号也可以报告I/O事件，可以为目标fd指定宿主进程，被指定的进程将捕获SIGIO信号，当fd有事件发生，SIGIO处理函数触发
// 
// 理论上，阻塞I/O、I/O复用和信号驱动I/O都是同步I/O模型，因为读写操作都是在I/O事件发生后由应用程序完成
// POSIX规范定义的异步I/O模型中，应该是用户可以直接对I/O操作执行读写，告诉内核自己读写缓冲区的位置，接下来就是内核接管
// 同步I/O通知给应用的是就绪事件，异步I/O向应用通知的是I/O完成事件 ***
//

//
//两种高效的事件处理模式
// 服务器通常需要处理三类事件，I/O事件、信号、定时事件
// 同步I/O模型通常用于实现Reactor模式
// 异步I/O模型则用于实现Proactor模式
// 也可以使用同步I/O方式模拟出Proactor模式
// 
//Reactor
// 要求主线程（I/O处理单元）只负责监听文件描述符上是否有事件发生
// 有就立即将该事件通知工作线程（逻辑单元），除此之外主线程不做其他实质性工作
// 读写、接收新连接、处理请求等在工作线程完成
// 
// 同步I/O模型实现Reactor模式的流程（使用epoll_wait为例）
//	主线程向epoll内核事件表中注册socket读就绪事件
//	主线程调用epoll_wait等待socket上有数据可读
//	触发事件后，epoll_wait通知主线程，主线程将可读事件放入请求队列
//	请求队列上某个工作线程被唤醒，从socket读取数据，处理客户请求，然后向epoll内核事件表注册socket写事件就绪事件
//	主线程epoll_wait等待socket可写，触发时将可写事件放入请求队列
//	请求队列某工作线程被唤醒，向socket写数据，返回请求结果
// 
//Proactor
// 与Reactor模式不同，将所有I/O操作都交给主线程和内核，工作线程仅负责业务逻辑
// 
// 异步I/O模型实现Proactor模式的流程（以aio_read、aio_write为例）
//	主线程调用aio_read函数向内核注册socket上的读完成事件，告知读缓冲区位置，以及读完成时的通知方式（信号）
//	主线程继续处理其他逻辑
//	当socket数据被读入用户缓冲区后，内核通知应用程序，表示数据可用
//	应用程序通过预先定义好的处理函数选择一个工作线程处理请求
//	工作线程完成处理后，调用aio_write函数向内核注册写完成事件，告知写缓冲区位置，以及写完成时的通知方式
//	主线程继续处理其他逻辑
//	当用户缓冲区数据写入socket后，内核通知应用程序，表示数据发送完毕
//	应用程序通过预先定义好的处理函数选择一个工作线程做善后处理（继续接收或关闭socket等）
// 
//同步I/O模拟Proactor模式
// 原理是主线程执行数据读写操作，完成后，主线程通知工作线程模拟“事件完成”
// 从工作线程的角度，就是直接获取了读写结果，只需要对读写结果进行逻辑处理
// 
// 使用同步I/O模型模拟Proactor模式的流程（以epoll_wait为例）
//	主线程向epoll内核事件表注册socket读就绪事件
//	主线程调用epoll_wait等待数据可读
//	触发事件后，epoll_wait通知主线程，主线程从socket循环读取数据，完全读完后将数据封装为请求对象插入请求队列
//	请求队列上某个工作线程被唤醒，拿到请求对象做业务处理，然后向epoll内核事件表注册socket写就绪事件
//	主线程epoll_wait等待socket可写
//	触发事件后，epoll_wait通知主线程，主线程向socket上写入请求队列处理后的请求结果
//

int main()
{
	std::cout << "Hello World!\n";
}
